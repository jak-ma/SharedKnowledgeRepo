# 调参知识

### 小数据集+预训练backbone+冻结策略

在做 Kaggle 比赛 CRISO 时，面对小数据集，采用 `dinov2-samll ` 模型作为 `强backbone ` 来做特征提取。

在实验的过程中，发现如此实验现象：

1. 当全epoch完全冻结，只依赖其余较小 `mlp` 模块学习时，很容易快速达到学习峰值，同出现模型能力没法进一步提升，`损失和评价指标 `在某个区间内来回徘徊。

2. 冻结前面5-10个epoch，（当时忘记调小微调时的backbone的学习率）导致将 backbone 解冻之后模型出现较大抖动，精度和loss瞬间上升。

3. 吸取2的经验，先进行一次实验，找到该`backbone` 下的性能极限epoch数，然后在此epoch前进行冻结backbone，此epoch之后采用较小学习率进行微调，如此便能将模型训练到较好水平。

   <img src="https://gitee.com/jak-ma/graph-s/raw/master/imgs/20251209202341066.png" alt="training_history_fold_1 (3)" style="zoom:33%;" />

基于上述实验总结，**模型微调是必要的**，不能只依赖backbone的强大提取能力，同时应该**注意分组学习率的设置，微调需要较小的学习率起步**。我认为可以做的更好的方向是，是对于分组学习率的设置更加精细化，如对于 backbone 的最后几层进行微调，同时采用逐层解冻的方式开展；基于的理论基础是**通常网络前面层是做通用特征表示，最后几层更加偏向特定任务表示。**